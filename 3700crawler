#!/usr/bin/env python3

import argparse, socket, ssl
from posixpath import split
from ast import parse

from html.parser import HTMLParser
from urllib.parse import urlparse


DEFAULT_SERVER = "project5.3700.network"
DEFAULT_LOGIN = "/accounts/login/"
DEFAULT_PORT = 443

# Nick's Login: 
#USERNAME = "coury.ni"
#PASSWORD = "001477153"

visited_pages = [] # Represents pages that we have visited
fronteir = [] # Represents pages that we are going to visit
flags = [] # Holds all discovered flags 
middleware_token = "" # Used for the login 

class MyHTMLParser(HTMLParser):
    # This gets all the links we need to process and adds them to pages_to_visit iff they haven't been visited
    #  tag a references a link -> href= LINK
    #  <a href= LINK>TEXT</a>
    def handle_starttag(self, tag, attrs):
        # We only care about links to other pages 
        global middleware_token
        if (tag == "a"):
            for attr in attrs:
                # attr -> (name, value) -> (href, link)
                if (attr[0] == "href" and attr[1] not in fronteir and attr[1] not in visited_pages):
                    fronteir.append(attr[1])
                    #print("ADDING " + str(attr[1]) + " TO FRONTEIR")

        # Example tag for middleware token:       
        #   <input type="hidden" name="csrfmiddlewaretoken" value="sP02rxK0K6gHIn4j8X2gmE77wa10WJPJjctTWZ65Xq79I6yxfSLO0kW95zvjykT8">
        # If we encounter the middleware, we know that the value is stored in the 3rd attribute
        #   Save it off
        elif(tag == "input"):
            for attr in attrs:
                if(attr[1] == "csrfmiddlewaretoken"):
                    middleware_token = "csrfmiddlewaretoken=" + attrs[2][1]
                    break

    # Handles the finding of flags and adds it to our flags list
    def handle_data(self, data):
        if "FLAG: " in data:
            flags.append(data[6:])


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password  
        self.html_parser = MyHTMLParser()
        self.location = ""
        self.csrf = ""
        self.sessionid = ""

    # Builds the cookie header based on the cookies we have and returns the finished cookie header
    def get_cookie(self):
        # Cookie: csrf=dasdasdasdasd; sessionid=asdasdasdasd
        add_semi = False
        cookie = ""
        if self.csrf != "" or self.sessionid != "":
            cookie = "Cookie: "
            if self.csrf:
                cookie += self.csrf
                add_semi = True
            if self.sessionid:
                if add_semi:
                    cookie += "; "
                cookie += self.sessionid

        # If not empty then add the trailing returns 
        if(cookie != ""):
            cookie += "\r\n"

        return cookie  

    # Returns the status code and stores information about cookies and location (if applicable)
    def parse_http(self, data):
        # First, get the status and turn the rest into a dictionary
        split_data = data.split("\n")
        headers = []
        i = 0
        for line in split_data:
            if i > 0: 
                headers.append(line)
            else:
                status_line = line.split(" ")
                status = status_line[1]
                #print(status)
            i += 1    

        # Store the csrf, sessionid, and location if they are present in the header
        for header in headers:
            if "Set-Cookie" in header:
                split_cookie = header.split(" ")
                for part in split_cookie:
                    if "csrftoken" in part:
                        self.csrf = part[0:len(part) - 1]
                    elif "sessionid" in part: 
                        self.sessionid = part[0:len(part) - 1]  
            elif "Location" in header:
                split_location = header.split(" ")
                self.location = split_location[1][0: len(split_location[1]) - 1]
        return status

    # Sends a formatted message and processes the response given by the server
    def send_and_handle(self, host, path, message):
        while True:  
            # Wrap the socket in ssl 
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            context = ssl.SSLContext()
            sock = context.wrap_socket(sock, server_hostname=self.server)

            # Connect to the server @ port
            sock.connect((self.server, self.port))
            
            # Sending the GET request over the previously wrapped socket 
            sock.send(message.encode())
            
            # Recieve a Message
            data = sock.recv(100000)
            
            # Once a response was received, close the socket 
            sock.shutdown(1)
            sock.close()
            
            # Split here into HTTP and HTML responses
            split_response = data.split(b"\r\n\r\n\n\n\n")
            http_response = split_response[0].decode()

            # Feed the data to our parser based on the status code
            status = self.parse_http(http_response)
            
            # If we have successfully reached the page 
            if status == "200":
                html_response = split_response[1].decode()
                self.html_parser.feed(html_response)
                break
            elif status == "302":
                self.get_req(host, self.location)
                break
            elif status == "403" or status == "404":
                break
            elif status == "500":
                continue
            else:
                raise Exception("Error Code Status: " + status)

        
  
    def get_req(self, host, path):  
        if self.server in host : 
            # Add path to the visited_pages list
            visited_pages.append(path)
            # Checking if this is the root of the Host
            if(path == ""):
                path = "/" 

            # netloc is the root url  
            # original URL: "scheme://netloc/path;parameters?query#fragment"
                    
            message = "GET %s HTTP/1.0\r\nHost: %s\r\n%s\r\n" % (path, host, self.get_cookie())
            self.send_and_handle(host, path, message)

    
    def post_req(self, host, path, payload):
        if self.server in host:
            if path == "":
                path = "/"

            # Add the header to the post message 
            post_msg = "POST %s HTTP/1.1\r\n" % path
            post_msg += "Host: %s\r\n" % host
            # Add the content length field representing the length of the payload 
            post_msg += "Content-Length: %s\r\n" % str(len(payload)) 
            # Add the content type (always application/x-www-form-urlencoded ?)
            post_msg += "Content-Type: application/x-www-form-urlencoded\r\n"
            # Add the cookie 
            post_msg += self.get_cookie() + "\r\n"
            # Add the data 
            post_msg += payload 
            self.send_and_handle(host, path, post_msg)        

    def login(self):
        visited_pages.append("/accounts/logout/")
        # Send the original get request 
        self.get_req(DEFAULT_SERVER, DEFAULT_LOGIN)
        # Make the payload (username, password, middleware token)
        payload = "username=%s&password=%s&%s&next=/fakebook/" % (self.username, self.password, middleware_token)
        # Post message a request 
        self.post_req(DEFAULT_SERVER, DEFAULT_LOGIN, payload)
    
    def run(self):      
        # Start by logging in
        self.login()
        
        host_server = urlparse(self.server)
        host_server_scheme = host_server.scheme
        host_server_netloc = host_server.netloc

        # Start the crawl
        while len(flags) <= 5 and len(fronteir) != 0:
            parsed_url = urlparse(fronteir[0])

            # Check if the domain of the current url is the same as the given domain 
            #   If both the scheme and netloc are the same
            #       If they are, then send a GET 
            #   If only the scheme is the same but different netloc's 
            #       Remove the bad url 
            #   If the schemes are different  
            #       Remove the bad url 
            if (parsed_url.netloc == host_server_netloc and parsed_url.scheme == host_server_scheme):
                print("Visting... ", fronteir[0])
                self.get_req(self.server, fronteir[0])
                fronteir.pop(0)
            else:
                print("removed bad url")
                fronteir.pop(0)
        print(flags)
            
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
