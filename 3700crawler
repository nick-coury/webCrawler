#!/usr/bin/env python3

import argparse, socket, ssl
from posixpath import split
from ast import parse

from html.parser import HTMLParser
from urllib.parse import urlparse


DEFAULT_SERVER = "project5.3700.network"
DEFAULT_LOGIN = "/accounts/login/"
DEFAULT_PORT = 443

# Nick's Login: 
# username: coury.ni
# password: 001477153

visited_pages = [] # Represents pages that we have visited
fronteir = [] # Represents pages that we are going to visit
flags = [] # Holds all discovered flags 
middleware_token = "" # Used for the login 

class MyHTMLParser(HTMLParser):
    # This gets all the links we need to process and adds them to pages_to_visit iff they haven't been visited
    #  tag a references a link -> href= LINK
    #  <a href= LINK>TEXT</a>
    def handle_starttag(self, tag, attrs):
        # We only care about links to other pages 
        global middleware_token
        if (tag == "a"):
            # Possibly leave this out later when we test
            for attr in attrs:
            # attr -> (name, value) -> (href, link)
                if (attr[0] == "href" and attr[1] not in fronteir and attr[1] not in visited_pages):
                    fronteir.append(attr[1])

        # Example tag for middleware token:       
        # <input type="hidden" name="csrfmiddlewaretoken" value="sP02rxK0K6gHIn4j8X2gmE77wa10WJPJjctTWZ65Xq79I6yxfSLO0kW95zvjykT8">
        
        # If we encounter the middleware, we know that the value is stored in the 3rd attribute
        #  Save it off
        elif(tag == "input"):
            for attr in attrs:
                if(attr[1] == "csrfmiddlewaretoken"):
                    middleware_token = "csrfmiddlewaretoken=" + attrs[2][1]
                    break


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password  
        self.html_parser = MyHTMLParser()
        self.location = ""
        self.scrf = ""
        self.sessionid = ""

    # Builds the cookie header based on the cookies we have and returns the finished cookie header
    def get_cookie(self):
        # Cookie: csrf=dasdasdasdasd; sessionid=asdasdasdasd
        add_semi = False
        cookie = ""
        if self.scrf != "" or self.sessionid != "":
            cookie = "Cookie: "
            if self.scrf:
                cookie += self.csrf
                add_semi = True
            if self.sessionid:
                if add_semi:
                    cookie += "; "
                cookie += self.sessionid

        # If not empty then add the trailing returns 
        if(cookie != ""):
            cookie += "\r\n"

        return cookie  

    # Returns the status code and stores information about cookies and location (if applicable)
    def parse_http(self, data):
        # First, get the status and turn the rest into a dictionary
        split_data = data.split("\n")
        headers = []
        i = 0
        for line in split_data:
            if i > 0: 
                headers.append(line)
            else:
                status_line = line.split(" ")
                status = status_line[1]
                print(status)
            i += 1    

        # Store the csrf, sessionid, and location if they are present in the header
        for header in headers:
            if "Set-Cookie" in header:
                split_cookie = header.split(" ")
                for part in split_cookie:
                    if "csrftoken" in part:
                        self.csrf = part[0:len(part) - 1]
                    elif "sessionid" in part: 
                        self.sessionid = part[0:len(part) - 1]  
            elif "Location" in header:
                split_location = header.split(" ")
                self.location = split_location[1]  
        
        return status

    def get_req(self, sock, host, path):        
        # Add path to the visited_pages list
        visited_pages.append(path)
        
        # Checking if this is the root of the Host
        if(path == " "):
            path = "/" 

        # netloc is the root url  
        # original URL: "scheme://netloc/path;parameters?query#fragment"
        print("HOST: %s\n" % host)

        #self.get_cookie()
        message = "GET %s HTTP/1.0\r\nHost: %s\r\n%s\r\n" % (path, host, self.cookie)
        print("THE GET MSG: %s" % message)

        done = False
        while not done :    
            # Sending the GET request over the previously wrapped socket 
            sock.send(message.encode())
            # Recieve a Message
            data = sock.recv(100000)
            
            # Split here into HTTP and HTML responses
            split_response = data.split(b"\r\n\r\n\n\n\n")
            http_response = split_response[0].decode()
            html_response = split_response[1].decode()


            # Feed the data to our parser based on the status code
            status = self.parse_http(http_response)
            
            # If we have successfully reached the page 
            if status == "200":
                self.html_parser.feed(html_response)
                done = True
            elif status == "302":
                path = self.location
            elif status == "403" or status == "404":
                done == True
        
        print("UPDATED MIDDLEWARE TOKEN: ", middleware_token)
        print("UPDATED FRONTEIR ", fronteir)

        return data
    

    def login(self, sock, username, password):
        on_login_page = False
        while not on_login_page:
            loginPage = self.get_req(sock, DEFAULT_SERVER, DEFAULT_LOGIN)
            on_login_page = True
            # add a helper that takes in a get request and returns a dictionary 
            # if loginPage['Status'] == "200":
            #     on_login_page = True

        # POST path_from_host HTTTP/1.1
        # Host: DEFAULT_URL
        # Cookie: COOKIE
        # Content Length: (Size of the arugments including \r\n)
        # username=___&password=___&crfmiddlewaretocken=___
        # Content-Type: application/x-www-form-urlencoded

        # login = "POST %s HTTP/1.0\r\n" % DEFAULT_LOGIN
        # header_list = ['Content-Type: application/x-www-form-urlencoded', 'Host: ' + DEFAULT_SERVER] 
        # content = "username={username}&password={password}&crfmifflewaretoken={token}&next=%2Ffakebook%2F\r\n"
        # # Connects to the server @ port 
        # sock.connect(DEFAULT_LOGIN)
    
    def run(self):      
        # Init the socket and wrap in SSL 
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        context = ssl.SSLContext()
        mysocket = context.wrap_socket(mysocket, server_hostname=self.server)

        mysocket.connect((self.server, self.port))

        self.login(mysocket, "coury.ni", "001477153")

        # Start the crawl


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
