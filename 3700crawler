#!/usr/bin/env python3

import argparse, socket, ssl

from html.parser import HTMLParser
from urllib.parse import urlparse

DEFAULT_SERVER = "project5.3700.network"
DEFAULT_LOGIN = "/accounts/login/"
DEFAULT_PORT = 443

# Nick's Login: 
# username: coury.ni
# password: 001477153
visited_pages = {}
fronteir = []
flags = []

class MyHTMLParser(HTMLParser):
    # This gets all the links we need to process and adds them to pages_to_visit iff they haven't been visited
    #  tag a references a link -> href= LINK
    #  <a href= LINK>TEXT</a>
    def handle_starttag(self, tag, attrs):
        # We only care about links to other pages 
        if (tag == "a"):
            # Possibly leave this out later when we test
            for attr in attrs:
            # attr -> (name, value) -> (href, link)
                if (attr[0] == "href" and attr[1] not in fronteir):
                    fronteir.append(attr[1])

class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password  
        self.html_parser = MyHTMLParser()

        self.cookie = ""

    def get_cookie(self):
        return
        
    def get_req (self, sock, url) :
        # First parse our url
        parsed_url = urlparse(url)
        url_path = parsed_url.path

        # Checking if this is the root of the Host
        # original URL: "scheme://netloc/path;parameters?query#fragment" if the path was empty then the new path needs / 
        if(url_path == ""):
            url_path = "/" 

        # netloc is the root url  
        # original URL: "scheme://netloc/path;parameters?query#fragment"
        host = parsed_url.netloc

        sock.connect(host, DEFAULT_PORT)

        self.get_cookie()
        message = "GET %s HTTP/1.0\nHost: %s\n%s\r\n\r\n" % (url_path, host, self.cookie)

        # Sending the GET request over the previously wrapped socket 
        sock.send(message)
        
        data = sock.recv(100000)
        data.decode('ascii')

        print("DATA WE GOT FROM A GET: %s\n" % data)
        return data

    

    def login (sock, username, password):
        # POST path_from_host HTTTP/1.1
        # Host: DEFAULT_URL
        # Cookie: COOKIE
        # Content Length: (Size of the arugments including \r\n)
        # username=___&password=___&crfmiddlewaretocken=___
        # Content-Type: application/x-www-form-urlencoded
        on_login_page = False
        while not on_login_page:
            loginPage = DEFAULT_SERVER #GET(LOGIN_ENDPOINT)
            if loginPage['Status'] == str(200):
                on_login_page = True

        token = loginPage['Cookies-Object']['csrftoken']
        session_id = loginPage['Cookies-Object']['sessionid']



        login = "POST %s HTTP/1.0\r\n" % DEFAULT_LOGIN
        header_list = ['Content-Type: application/x-www-form-urlencoded', 'Host: ' + DEFAULT_SERVER] 
        content = "username={username}&password={password}&crfmifflewaretoken={token}&next=%2Ffakebook%2F\r\n"
        # Connects to the server @ port 
        sock.connect(DEFAULT_LOGIN)

        # Send the original GET message 
        sock.send(login.encode('ascii'))
    
    def run(self):      
        # Init the socket and wrap in SSL 
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        context = ssl.SSLContext()
        mysocket = context.wrap_socket(mysocket, server_hostname=self.server)

        # Connects to the server @ port 
        mysocket.connect((self.server, self.port))

        # Send the original GET message 
        response = self.get_req(mysocket, DEFAULT_SERVER)
        print("RESPONSE FROM GET: %s\n" % response)

        # Get the HTML structure of the login page in order to get the session crf 
        # html_request = "GET / HTTP/1.0\r\n\r\n"
        # print("HTML request to %s:%d" % (self.server, self.port))
        # print(html_request)
        # print("------------\n")

        # print("before waiting for data\n")
        # # Get the HTML request response 
        # data = mysocket.recv(1000)
        # print("received data\n")
        # print("HTML response:\n%s" % data.decode('ascii'))
        # print("------------\n")
        # print("after waiting for data\n")

        # Send a POST message with the username and password 
        #post_msg = self.gen_post_msg(self.username, self.password, token)
         
        # mysocket.send(request.encode('ascii'))

        # done = False

        # #while not done: 
        # #for i in range (4) :
        #     #print ("[%d]" % i)
        # data = mysocket.recv(1000)
        # print("Response:\n%s" % data.decode('ascii'))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
