#!/usr/bin/env python3

import argparse, socket, ssl
from posixpath import split
from ast import parse

from html.parser import HTMLParser
from urllib.parse import urlparse


DEFAULT_SERVER = "project5.3700.network"
DEFAULT_LOGIN = "/accounts/login/"
DEFAULT_PORT = 443

# Nick's Login: 
#USERNAME = "coury.ni"
#PASSWORD = "001477153"

visited_pages = [] # Represents pages that we have visited
fronteir = [] # Represents pages that we are going to visit
flags = [] # Holds all discovered flags 
middleware_token = "" # Used for the login 

class MyHTMLParser(HTMLParser):
    # This gets all the links we need to process and adds them to pages_to_visit iff they haven't been visited
    #  tag a references a link -> href= LINK
    #  <a href= LINK>TEXT</a>
    def handle_starttag(self, tag, attrs):
        # We only care about links to other pages 
        global middleware_token
        if (tag == "a"):
            # Possibly leave this out later when we test
            for attr in attrs:
            # attr -> (name, value) -> (href, link)
                if (attr[0] == "href" and attr[1] not in fronteir and attr[1] not in visited_pages):
                    fronteir.append(attr[1])

        # Example tag for middleware token:       
        # <input type="hidden" name="csrfmiddlewaretoken" value="sP02rxK0K6gHIn4j8X2gmE77wa10WJPJjctTWZ65Xq79I6yxfSLO0kW95zvjykT8">
        
        # If we encounter the middleware, we know that the value is stored in the 3rd attribute
        #  Save it off
        elif(tag == "input"):
            for attr in attrs:
                if(attr[1] == "csrfmiddlewaretoken"):
                    middleware_token = "csrfmiddlewaretoken=" + attrs[2][1]
                    break


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password  
        self.html_parser = MyHTMLParser()
        self.location = ""
        self.csrf = ""
        self.sessionid = ""

    # Builds the cookie header based on the cookies we have and returns the finished cookie header
    def get_cookie(self):
        # Cookie: csrf=dasdasdasdasd; sessionid=asdasdasdasd
        add_semi = False
        cookie = ""
        if self.csrf != "" or self.sessionid != "":
            cookie = "Cookie: "
            if self.csrf:
                cookie += self.csrf
                add_semi = True
            if self.sessionid:
                if add_semi:
                    cookie += "; "
                cookie += self.sessionid

        # If not empty then add the trailing returns 
        if(cookie != ""):
            cookie += "\r\n"

        return cookie  

    # Returns the status code and stores information about cookies and location (if applicable)
    def parse_http(self, data):
        # First, get the status and turn the rest into a dictionary
        split_data = data.split("\n")
        headers = []
        i = 0
        for line in split_data:
            if i > 0: 
                headers.append(line)
            else:
                status_line = line.split(" ")
                status = status_line[1]
                print(status)
            i += 1    

        # Store the csrf, sessionid, and location if they are present in the header
        for header in headers:
            if "Set-Cookie" in header:
                split_cookie = header.split(" ")
                for part in split_cookie:
                    if "csrftoken" in part:
                        self.csrf = part[0:len(part) - 1]
                    elif "sessionid" in part: 
                        self.sessionid = part[0:len(part) - 1]  
            elif "Location" in header:
                split_location = header.split(" ")
                self.location = split_location[1]  
        
        print("CSRF ", self.csrf)
        print("COOKIES ", self.get_cookie())

        return status

    def send_and_handle(self, host, path, message):
        done = False
        while not done:  
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            context = ssl.SSLContext()
            sock = context.wrap_socket(sock, server_hostname=self.server)

            sock.connect((self.server, self.port))
            
            # Sending the GET request over the previously wrapped socket 
            print("SENDING MESSAGE 11111111111111111111 \n", message)
            sock.send(message.encode())
            # Recieve a Message
            data = sock.recv(100000)
            # print("######## ", data)
            # Split here into HTTP and HTML responses
            split_response = data.split(b"\r\n\r\n\n\n\n")
            # print("$$$$$$$$ ", split_response)
            http_response = split_response[0].decode()

            # Feed the data to our parser based on the status code
            status = self.parse_http(http_response)
            
            # If we have successfully reached the page 
            if status == "200":
                html_response = split_response[1].decode()
                self.html_parser.feed(html_response)
                done = True
            elif status == "302":
                print("HOST &&&& : ", host)
                print("LOCATION &&&&& : ", self.location)
                self.get_req(host, self.location)
                done = True
            elif status == "403" or status == "404":
                done = True
            done = True
            
            sock.shutdown(1)
            sock.close()

        print(http_response)
        print(html_response)
        
    
    def get_req(self, host, path):        
        # Add path to the visited_pages list
        visited_pages.append(path)
        
        # Checking if this is the root of the Host
        if(path == " "):
            path = "/" 

        # netloc is the root url  
        # original URL: "scheme://netloc/path;parameters?query#fragment"
        print("HOST: %s\n" % host)

        #self.get_cookie()
        message = "GET %s HTTP/1.0\r\nHost: %s\r\n%s\r\n" % (path, host, self.get_cookie())
        print("THE GET MSG: %s" % message)

        self.send_and_handle(host, path, message)
    
    def post_req(self, host, path, payload):
        if path == "":
            path = "/"
        
        post_msg = ""

        # Add the header to the post message 
        post_msg += "POST %s HTTP/1.0\r\n" % path
        post_msg += "Host: %s\r\n" % host
        # Add the content length field representing the length of the payload 
        post_msg += "Content-Length: %s\r\n" % str(len(payload)) 
        # Add the content type (always application/x-www-form-urlencoded ?)
        post_msg += "Content-Type: application/x-www-form-urlencoded\r\n"
        # Add the cookie 
        post_msg += self.get_cookie() + "\r\n"
        # Add the data 
        post_msg += payload 
        self.send_and_handle(host, path, post_msg)        

    def login(self):
        # Send the original get request 
        self.get_req(DEFAULT_SERVER, DEFAULT_LOGIN)
        # Make the payload (username, password, middleware token)
        payload = "username=%s&password=%s&%s&next=/fakebook/" % (self.username, self.password, middleware_token)
        # Post message a request 
        self.post_req(DEFAULT_SERVER, DEFAULT_LOGIN, payload)
    
    def run(self):      
        # Init the socket and wrap in SSL 
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        context = ssl.SSLContext()
        mysocket = context.wrap_socket(mysocket, server_hostname=self.server)

        mysocket.connect((self.server, self.port))

        self.login()

        # Start the crawl


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
